{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff704e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, n_episodes=1000, gamma=0.99, lr=1e-4, batch_size=64, \n",
    "          buffer_size=100_000, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "          target_update_freq=10):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_size)\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset(seed=SEED)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                    action = policy_net(state_tensor).argmax().item()\n",
    "\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards_batch, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards_batch = torch.FloatTensor(rewards_batch).unsqueeze(1).to(device)\n",
    "                next_states = torch.FloatTensor(next_states).to(device)\n",
    "                dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                expected_q = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, expected_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            mean_reward = np.mean(rewards_per_episode[-10:])\n",
    "            print(f\"Episode {episode}, Mean Reward (last 10): {mean_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return policy_net, rewards_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy_net, n_episodes=20):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset(seed=SEED)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = torch.argmax(policy_net(state_tensor)).item()\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    print(f\"Average evaluation reward over {n_episodes} episodes: {avg_reward:.2f}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "policy_net, rewards = train(env, n_episodes=500)\n",
    "evaluate(env, policy_net)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training Rewards\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
